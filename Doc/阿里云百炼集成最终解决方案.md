# 阿里云百炼集成最终解决方案

## 问题诊断

### 测试结果
1. ✅ **直接 API 调用成功**：使用 PowerShell 直接调用阿里云百炼 API 成功
   ```
   model: deepseek-r1
   usage: @{prompt_tokens=4; completion_tokens=281; total_tokens=285}
   ```

2. ❌ **LangChain 集成失败**：
   - 第一次错误：`401 Incorrect API key provided` (OpenAI 提供商)
   - 第二次错误：`403 "unauthorized"` (Fireworks 提供商)

### 根本原因

根据[阿里云百炼控制台文档](https://bailian.console.aliyun.com/?spm=5176.29597918.resourceCenter.1.6d7d7b08wQgaGB&tab=api#/api/?type=model&url=2868565)，正确的调用方式应该是：

```javascript
const openai = new OpenAI({
    apiKey: process.env.DASHSCOPE_API_KEY, 
    baseURL: 'https://dashscope.aliyuncs.com/compatible-mode/v1'
});
```

**问题在于**：
- LangChain 的提供商（OpenAI、Fireworks 等）都会在 `baseUrl` 后面自动添加路径
- OpenAI 提供商：添加 `/chat/completions`
- Fireworks 提供商：可能添加 `/inference/v1/chat/completions` 或其他路径

## 解决方案一：创建自定义的 LangChain ChatModel

由于标准提供商都无法完全控制 URL 构建过程，我们需要创建一个自定义的 ChatModel：

```typescript
// apps/agents/src/utils.ts

import { ChatOpenAI } from "@langchain/openai";

// 为阿里云百炼创建自定义配置
if (customModelName.includes("fireworks/")) {
  let actualModelName = providerConfig.modelName;
  
  // 模型名称映射
  if (actualModelName.includes("deepseek-r1")) {
    actualModelName = "deepseek-r1";
  } else if (actualModelName.includes("deepseek-v3")) {
    actualModelName = "deepseek-v3";
  }
  
  // 直接使用 ChatOpenAI 并完全自定义配置
  const model = new ChatOpenAI({
    modelName: actualModelName,
    openAIApiKey: process.env.FIREWORKS_API_KEY,
    configuration: {
      baseURL: "https://dashscope.aliyuncs.com/compatible-mode/v1",
    },
    temperature: 0.5,
    maxTokens: 4096,
  });
  
  return model;
}
```

## 解决方案二：使用环境变量覆盖默认端点

修改环境变量来覆盖 OpenAI 的默认端点：

```bash
# .env 文件
OPENAI_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1
OPENAI_API_KEY=sk-5f7a6e663fab4487bbc16d3a5c7e1912
```

然后在代码中使用 OpenAI 提供商：

```typescript
return {
  ...providerConfig,
  modelName: actualModelName,
  modelProvider: "openai",
  apiKey: process.env.OPENAI_API_KEY,
  baseUrl: process.env.OPENAI_API_BASE,
};
```

## 解决方案三：修改 getModelFromConfig 函数

直接在 `getModelFromConfig` 函数中处理阿里云百炼的特殊情况：

```typescript
export async function getModelFromConfig(
  config: LangGraphRunnableConfig,
  extra?: {
    temperature?: number;
    maxTokens?: number;
    isToolCalling?: boolean;
  }
): Promise<ReturnType<typeof initChatModel>> {
  const {
    modelName,
    modelProvider,
    azureConfig,
    apiKey,
    baseUrl,
    modelConfig,
  } = getModelConfig(config, {
    isToolCalling: extra?.isToolCalling,
  });

  // 特殊处理阿里云百炼
  if (baseUrl?.includes("dashscope.aliyuncs.com")) {
    const { ChatOpenAI } = await import("@langchain/openai");
    return new ChatOpenAI({
      modelName,
      openAIApiKey: apiKey,
      configuration: {
        baseURL: baseUrl,
      },
      temperature: extra?.temperature ?? 0.5,
      maxTokens: extra?.maxTokens,
    });
  }

  // 原有的 initChatModel 逻辑
  return await initChatModel(modelName, {
    modelProvider,
    // ... 其他参数
  });
}
```

## 推荐方案：方案三（最小侵入性）

方案三是最推荐的，因为：
1. ✅ 最小程度修改现有代码
2. ✅ 不影响其他提供商
3. ✅ 完全控制阿里云百炼的 API 调用
4. ✅ 易于维护和扩展

## 实施步骤

### 步骤 1：修改 getModelFromConfig 函数

在 `apps/agents/src/utils.ts` 的 `getModelFromConfig` 函数开头添加：

```typescript
export async function getModelFromConfig(
  config: LangGraphRunnableConfig,
  extra?: {
    temperature?: number;
    maxTokens?: number;
    isToolCalling?: boolean;
  }
): Promise<ReturnType<typeof initChatModel>> {
  const {
    modelName,
    modelProvider,
    azureConfig,
    apiKey,
    baseUrl,
    modelConfig,
  } = getModelConfig(config, {
    isToolCalling: extra?.isToolCalling,
  });
  
  const { temperature = 0.5, maxTokens } = {
    temperature: modelConfig?.temperatureRange.current,
    maxTokens: modelConfig?.maxTokens.current,
    ...extra,
  };

  // 🔧 特殊处理阿里云百炼
  if (baseUrl?.includes("dashscope.aliyuncs.com")) {
    const { ChatOpenAI } = await import("@langchain/openai");
    return new ChatOpenAI({
      modelName,
      openAIApiKey: apiKey,
      configuration: {
        baseURL: baseUrl,
      },
      temperature,
      maxTokens,
    }) as any;
  }

  // ... 原有代码继续
}
```

### 步骤 2：确保 baseUrl 正确

在 `getModelConfig` 函数中：

```typescript
if (customModelName.includes("fireworks/")) {
  let actualModelName = providerConfig.modelName;
  
  // 模型名称映射
  if (actualModelName.includes("deepseek-r1")) {
    actualModelName = "deepseek-r1";
  } else if (actualModelName.includes("deepseek-v3")) {
    actualModelName = "deepseek-v3";
  }
  
  return {
    ...providerConfig,
    modelName: actualModelName,
    modelProvider: "fireworks", // 这个值不重要，因为会被特殊处理
    apiKey: process.env.FIREWORKS_API_KEY,
    baseUrl: "https://dashscope.aliyuncs.com/compatible-mode/v1", // 基础 URL
  };
}
```

### 步骤 3：环境变量配置

```bash
# .env 文件
FIREWORKS_API_KEY=sk-5f7a6e663fab4487bbc16d3a5c7e1912
NEXT_PUBLIC_FIREWORKS_ENABLED=true
```

### 步骤 4：重启服务

```bash
# 停止服务
taskkill /F /IM node.exe

# 重新构建
cd apps/agents && yarn build

# 启动服务
cd apps/agents && yarn dev
cd apps/web && yarn dev
```

## 测试验证

### 1. 直接 API 测试
```powershell
$headers = @{
    "Authorization" = "Bearer sk-5f7a6e663fab4487bbc16d3a5c7e1912"
    "Content-Type" = "application/json"
}

$body = @{
    model = "deepseek-r1"
    messages = @(
        @{
            role = "user"
            content = "你是谁"
        }
    )
    stream = $false
} | ConvertTo-Json -Depth 3

Invoke-RestMethod -Uri "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions" -Method POST -Headers $headers -Body $body
```

### 2. Open Canvas 测试
1. 打开 `http://localhost:3000`
2. 选择 Fireworks 分组下的 DeepSeek R1 模型
3. 发送消息测试

## 总结

核心问题是 LangChain 的标准提供商无法完全自定义 URL 构建过程。通过在 `getModelFromConfig` 函数中添加特殊处理逻辑，直接使用 `ChatOpenAI` 类并完全控制配置，可以完美解决这个问题。

这个方案：
- ✅ 保持了代码的整洁性
- ✅ 不影响其他模型提供商
- ✅ 完全控制阿里云百炼的 API 调用
- ✅ 易于调试和维护
