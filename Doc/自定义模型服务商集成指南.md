# Open Canvas 自定义模型服务商集成指南

## 概述

本文档基于对 [LangGraph](https://langgraph.com.cn/index.html) 和 [LangChain](https://python.langchain.ac.cn/docs/introduction/) 官方文档的深入分析，结合 Open Canvas 项目的实际代码结构，详细说明如何在 Open Canvas 中集成自定义模型服务商。

## 1. 架构基础

### 1.1 LangChain 与 LangGraph 协作模式

根据官方文档分析，LangChain 和 LangGraph 采用**分层协作**模式：

- **LangChain**: 提供基础的 LLM 交互能力
  - 模型调用和配置
  - 工具绑定和调用
  - 消息处理和格式化
  - 多提供商支持

- **LangGraph**: 提供复杂工作流编排能力
  - 状态管理和持久化
  - 节点路由和条件分支
  - 并行执行和循环控制
  - 可视化调试和监控

### 1.2 Open Canvas 中的实现

```
┌─────────────────────────────────────┐
│           Open Canvas App           │
├─────────────────────────────────────┤
│           LangGraph                 │ ← 工作流编排层
│  ┌─────────────────────────────────┐ │
│  │        LangGraph Nodes          │ │
│  │  ┌─────────────────────────────┐ │ │
│  │  │        LangChain            │ │ │ ← LLM 交互层
│  │  │  - Model Configuration     │ │ │
│  │  │  - Tool Binding            │ │ │
│  │  │  - Message Handling        │ │ │
│  │  │  - API Calls               │ │ │
│  │  └─────────────────────────────┘ │ │
│  └─────────────────────────────────┘ │
└─────────────────────────────────────┘
```

## 2. 自定义模型服务商集成方法

### 2.1 方法一：使用现有提供商的自定义 baseUrl

这是最简单的方法，适用于兼容 OpenAI API 格式的服务商。

#### 2.1.1 实现步骤

1. **修改 `getModelConfig` 函数**

在 `apps/agents/src/utils.ts` 中为现有提供商添加自定义 baseUrl：

```typescript
// 示例：为 OpenAI 提供商添加自定义 baseUrl
if (
  customModelName.includes("gpt-") ||
  customModelName.includes("o1") ||
  customModelName.includes("o3")
) {
  let actualModelName = providerConfig.modelName;
  if (extra?.isToolCalling && actualModelName.includes("o1")) {
    actualModelName = "gpt-4o";
  }
  return {
    ...providerConfig,
    modelName: actualModelName,
    modelProvider: "openai",
    apiKey: process.env.OPENAI_API_KEY,
    baseUrl: process.env.OPENAI_CUSTOM_BASE_URL, // 添加自定义 baseUrl
  };
}
```

2. **配置环境变量**

```bash
# 自定义 OpenAI 端点
OPENAI_CUSTOM_BASE_URL=https://your-custom-api.com/v1
OPENAI_API_KEY=your_custom_api_key
```

#### 2.1.2 适用场景

- 使用 OpenAI 兼容的 API 服务
- 通过代理服务器访问模型
- 使用本地部署的模型服务

### 2.2 方法二：创建新的模型提供商

对于不兼容现有提供商格式的服务商，需要创建新的提供商配置。

#### 2.2.1 实现步骤

1. **在 `getModelConfig` 函数中添加新的提供商**

```typescript
// 添加自定义提供商
if (customModelName.startsWith("custom/")) {
  const actualModelName = customModelName.replace("custom/", "");
  return {
    modelName: actualModelName,
    modelProvider: "openai", // 使用最接近的兼容提供商
    apiKey: process.env.CUSTOM_API_KEY,
    baseUrl: process.env.CUSTOM_API_URL,
  };
}
```

2. **在 `packages/shared/src/models.ts` 中定义模型**

```typescript
const CUSTOM_MODELS: ModelConfigurationParams[] = [
  {
    name: "custom/model-name",
    label: "Custom Model",
    config: {
      provider: "custom",
      temperatureRange: {
        min: 0,
        max: 1,
        default: 0.5,
        current: 0.5,
      },
      maxTokens: {
        min: 1,
        max: 8000,
        default: 4096,
        current: 4096,
      },
    },
    isNew: false,
  },
];

// 添加到 ALL_MODELS 数组
export const ALL_MODELS: ModelConfigurationParams[] = [
  ...OPENAI_MODELS,
  ...ANTHROPIC_MODELS,
  ...FIREWORKS_MODELS,
  ...GEMINI_MODELS,
  ...AZURE_MODELS,
  ...OLLAMA_MODELS,
  ...GROQ_MODELS,
  ...CUSTOM_MODELS, // 添加自定义模型
];
```

3. **配置环境变量**

```bash
# 自定义模型服务商配置
CUSTOM_API_KEY=your_custom_api_key
CUSTOM_API_URL=https://your-custom-service.com/api/v1
NEXT_PUBLIC_CUSTOM_ENABLED=true
```

#### 2.2.2 适用场景

- 使用完全自定义的 API 格式
- 需要特殊的认证方式
- 使用非标准的模型参数

### 2.3 方法三：修改现有提供商配置

直接修改现有提供商的配置，替换其默认端点。

#### 2.3.1 实现步骤

1. **修改现有提供商配置**

```typescript
// 修改 Fireworks 提供商使用自定义端点
if (customModelName.includes("fireworks/")) {
  let actualModelName = providerConfig.modelName;
  if (
    extra?.isToolCalling &&
    actualModelName !== "accounts/fireworks/models/llama-v3p3-70b-instruct"
  ) {
    actualModelName = "accounts/fireworks/models/llama-v3p3-70b-instruct";
  }
  return {
    ...providerConfig,
    modelName: actualModelName,
    modelProvider: "fireworks",
    apiKey: process.env.FIREWORKS_API_KEY,
    baseUrl: process.env.FIREWORKS_CUSTOM_URL, // 使用自定义端点
  };
}
```

2. **配置环境变量**

```bash
# 使用自定义端点替换默认端点
FIREWORKS_API_KEY=your_custom_api_key
FIREWORKS_CUSTOM_URL=https://your-custom-service.com/inference/v1
```

#### 2.3.2 适用场景

- 替换现有提供商的默认服务
- 使用兼容的替代服务
- 临时切换到不同的服务提供商

## 3. 实际案例：阿里云百炼集成

### 3.1 案例背景

将阿里云百炼（DashScope）集成到 Open Canvas 中，替换原有的 Fireworks 提供商。

### 3.2 实现方案

#### 3.2.1 修改 Fireworks 提供商配置

```typescript
// apps/agents/src/utils.ts
if (customModelName.includes("fireworks/")) {
  let actualModelName = providerConfig.modelName;
  
  // 将 Fireworks 模型名称转换为阿里云百炼格式
  if (actualModelName.includes("deepseek-r1")) {
    actualModelName = "deepseek-r1";
  } else if (actualModelName.includes("deepseek-v3")) {
    actualModelName = "deepseek-v3";
  } else if (actualModelName.includes("llama-v3p3-70b-instruct")) {
    actualModelName = "llama-v3p3-70b-instruct";
  }
  
  if (
    extra?.isToolCalling &&
    actualModelName !== "llama-v3p3-70b-instruct" &&
    actualModelName !== "deepseek-r1"
  ) {
    actualModelName = "llama-v3p3-70b-instruct";
  }
  
  return {
    ...providerConfig,
    modelName: actualModelName,
    modelProvider: "fireworks", // 保持原有提供商
    apiKey: process.env.FIREWORKS_API_KEY, // 使用阿里云密钥
    baseUrl: "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions", // 阿里云端点
  };
}
```

#### 3.2.2 环境变量配置

```bash
# 阿里云百炼配置
FIREWORKS_API_KEY=sk-your-dashscope-api-key
FIREWORKS_API_URL=https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions
NEXT_PUBLIC_FIREWORKS_ENABLED=true
```

#### 3.2.3 模型标签更新

```typescript
// packages/shared/src/models.ts
const FIREWORKS_MODELS: ModelConfigurationParams[] = [
  {
    name: "accounts/fireworks/models/deepseek-r1",
    label: "DeepSeek R1 (DashScope)", // 添加标识
    config: {
      provider: "fireworks",
      temperatureRange: {
        min: 0,
        max: 1,
        default: 0.5,
        current: 0.5,
      },
      maxTokens: {
        min: 1,
        max: 8000,
        default: 4096,
        current: 4096,
      },
    },
    isNew: false,
  },
  // ... 其他模型
];
```

### 3.3 关键要点

1. **保持兼容性**: 使用现有的 `modelProvider: "fireworks"` 确保前端无需修改
2. **模型名称映射**: 将 Fireworks 格式的模型名称转换为阿里云百炼格式
3. **工具调用支持**: 保持工具调用回退机制
4. **环境变量管理**: 通过环境变量灵活配置 API 密钥和端点

## 4. 提供商选择指南

### 4.1 各提供商特点对比

| 提供商 | modelProvider | 默认端点 | 自定义 baseUrl | 工具调用支持 | 适用场景 |
|--------|---------------|----------|----------------|--------------|----------|
| OpenAI | "openai" | `https://api.openai.com/v1` | ✅ | ✅ | OpenAI 兼容 API |
| Anthropic | "anthropic" | `https://api.anthropic.com` | ❌ | ✅ | Anthropic 原生 API |
| Google | "google-genai" | `https://generativelanguage.googleapis.com/v1beta` | ❌ | ✅ | Google Gemini API |
| Fireworks | "fireworks" | `https://api.fireworks.ai/inference/v1` | ❌ | ✅ | Fireworks 原生 API |
| Groq | "groq" | `https://api.groq.com/openai/v1` | ❌ | ✅ | Groq 原生 API |
| Azure OpenAI | "azure_openai" | Azure 配置端点 | ✅ | ✅ | Azure OpenAI 服务 |
| Ollama | "ollama" | 需要自定义 | ✅ | ❌ | 本地部署模型 |

### 4.2 选择建议

1. **OpenAI 兼容 API**: 使用 `modelProvider: "openai"` + 自定义 `baseUrl`
2. **完全自定义 API**: 创建新的提供商配置
3. **替换现有服务**: 修改现有提供商的 `baseUrl`
4. **本地部署**: 使用 `modelProvider: "ollama"` + 自定义 `baseUrl`

## 5. 最佳实践

### 5.1 配置管理

1. **环境变量优先**: 使用环境变量管理 API 密钥和端点
2. **配置验证**: 在启动时验证必要的环境变量
3. **错误处理**: 为 API 调用失败提供适当的错误处理

### 5.2 模型名称管理

1. **统一格式**: 保持模型名称格式的一致性
2. **名称映射**: 在需要时进行模型名称转换
3. **标签更新**: 更新模型标签以反映实际服务商

### 5.3 工具调用支持

1. **回退机制**: 为不支持工具调用的模型提供回退方案
2. **兼容性检查**: 确保自定义服务商支持工具调用
3. **测试验证**: 充分测试工具调用功能

### 5.4 错误处理

1. **API 错误**: 处理各种 API 错误情况
2. **网络错误**: 处理网络连接问题
3. **认证错误**: 处理 API 密钥无效等问题

## 6. 故障排除

### 6.1 常见问题

1. **API 密钥错误**
   - 检查环境变量是否正确设置
   - 确认 API 密钥在服务商控制台中有效

2. **端点配置错误**
   - 验证 `baseUrl` 格式是否正确
   - 确认端点是否可访问

3. **模型名称不匹配**
   - 检查模型名称映射是否正确
   - 确认服务商支持相应的模型

4. **工具调用失败**
   - 检查服务商是否支持工具调用
   - 验证工具调用格式是否正确

### 6.2 调试方法

1. **日志输出**: 添加详细的日志输出
2. **API 测试**: 使用 curl 等工具直接测试 API
3. **环境检查**: 验证环境变量是否正确加载

## 7. 总结

通过以上方法，可以在 Open Canvas 中成功集成自定义模型服务商：

1. **方法一**适用于 OpenAI 兼容的 API 服务
2. **方法二**适用于完全自定义的 API 格式
3. **方法三**适用于替换现有服务商

关键成功因素：
- 正确理解 LangChain 和 LangGraph 的协作模式
- 选择合适的提供商配置方法
- 正确配置环境变量和模型参数
- 充分测试和验证集成效果

通过遵循本指南，开发者可以灵活地在 Open Canvas 中集成各种自定义模型服务商，同时保持系统的稳定性和可维护性。
